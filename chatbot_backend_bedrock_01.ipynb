{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 import the OS, Bedrock, ConversationChain, ConversationBufferMemory Langchain Modules\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "#2a Write a function for invoking model- client connection with Bedrock with profile, model_id & Inference params- model_kwargs\n",
    "# def demo_chatbot():\n",
    "def demo_chatbot(input_text):\n",
    "    demo_llm = Bedrock(\n",
    "       credentials_profile_name='default',\n",
    "       model_id='mistral.mixtral-8x7b-instruct-v0:1',\n",
    "       model_kwargs= {\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_p\": 0.5,\n",
    "        \"max_gen_len\": 512})\n",
    "    # return demo_llm\n",
    "    \n",
    "#2b Test out the LLM with Predict method\n",
    "    return demo_llm.invoke(input_text)\n",
    "# response = demo_chatbot('what is the temprature in london like ?')\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for conversation history; in a real application, this would be more dynamically managed\n",
    "conversation_history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "def generate_response(user_input):\n",
    "    \"\"\"\n",
    "    Generate a response from the model based on the user's input and the conversation history.\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "    # Add the latest user input to the conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Prepare the prompt by including the conversation history\n",
    "    prompt_with_history = \"\"\n",
    "    for entry in conversation_history:\n",
    "        prompt_with_history += f\"{entry['role']}: {entry['content']}\\n\"\n",
    "    prompt_with_history += \"assistant:\"\n",
    "\n",
    "    # Request to the endpoint\n",
    "    # response = predictor.predict(request)\n",
    "    response = demo_chatbot(prompt_with_history)\n",
    "\n",
    "    # Extracting model response\n",
    "    # model_response = response[0][\"generated_text\"].split(\"assistant:\")[-1].strip()\n",
    "    model_response = response.split(\"assistant:\")[-1].strip()\n",
    "\n",
    "    # Add model response to the conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": model_response})\n",
    "\n",
    "    # Return the model's response\n",
    "    return model_response\n",
    "\n",
    "# Example usage\n",
    "user_input = \"What is the capital of Spain?\"\n",
    "assistant_response = generate_response(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Spain is Madrid. Would you like to know about the history or culture of Madrid?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Spain is Madrid. Would you like to know about the history or culture of Madrid?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(assistant_response)\n",
    "assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most famous street in Madrid is the Gran Via. It's a bustling thoroughfare filled with shops, restaurants, hotels, and theaters. It's often compared to Broadway in New York City or the Champs-Élysées in Paris. Is there anything specific you would like to know about the Gran Via?\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What is the most famous street in Madrid?\"\n",
    "assistant_response = generate_response(user_input)\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most famous building on Gran Via Street in Madrid is the Metropolis Building. It's a beautiful Beaux-Arts style structure that was completed in 1911. The building is known for its stunning dome and the sculptures of the goddess Victoria that adorn the top. Would you like to know more about the Metropolis Building or any other building on Gran Via Street?\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What is the most famous house in Gran Via Street in Madrid?\"\n",
    "assistant_response = generate_response(user_input)\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked about Spain. Is there anything else you would like to know about Spain or its capital, Madrid? I'm here to help!\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What country did I ask about above?\"\n",
    "assistant_response = generate_response(user_input)\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
