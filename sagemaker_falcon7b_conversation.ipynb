{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YOrv5cYp6Va",
        "outputId": "6087dec1-266c-44e9-ee64-e698166aad31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install sagemaker python-dotenv --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ezunq2KXqAUf",
        "outputId": "b44263fb-f2f9-4675-92b5-b1d405ac8ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
            "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# environment variables\n",
        "# Option 1\n",
        "# os.environ[\"aws_access_key_id\"]='aws_access_key_id'\n",
        "# os.environ[\"aws_secret_access_key\"]='aws_secret_access_key'\n",
        "\n",
        "# Option 2\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYTO-4F4qDJp",
        "outputId": "24c1d136-8e13-448d-8d8c-043064a0db96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REGION_NAME = \"us-east-1\"\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] = REGION_NAME\n",
        "ROLE_NAME =  'Sagemaker-ExecutionRole'\n",
        "\n",
        "auth_arguments = {\n",
        "    'aws_access_key_id':os.environ[\"aws_access_key_id\"],\n",
        "    'aws_secret_access_key':os.environ[\"aws_secret_access_key\"],\n",
        "    'region_name':REGION_NAME\n",
        "}\n"
      ],
      "metadata": {
        "id": "Jqw9qUQ-qFVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iam = boto3.client('iam', **auth_arguments)\n",
        "role = iam.get_role(RoleName=ROLE_NAME)['Role']['Arn']\n",
        "\n",
        "session = sagemaker.Session(boto3.Session(**auth_arguments))\n"
      ],
      "metadata": {
        "id": "Q5pnL-qnqIA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
        "\n",
        "# image uri\n",
        "llm_image = get_huggingface_llm_image_uri(\"huggingface\")\n",
        "\n",
        "print(f\"image uri: {llm_image}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqkNh4yjqKep",
        "outputId": "276241cb-4701-43fd-e67d-ea1909fe2e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.2-gpu-py310-cu121-ubuntu22.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.huggingface import HuggingFaceModel\n",
        "\n",
        "# Falcon 7b\n",
        "hub = {'HF_MODEL_ID':'tiiuae/falcon-7b'}\n",
        "\n",
        "# Hugging Face Model Class\n",
        "huggingface_model = HuggingFaceModel(\n",
        "   env=hub,\n",
        "   role=role,  # iam role from AWS\n",
        "   image_uri=llm_image,\n",
        "   sagemaker_session=session\n",
        ")"
      ],
      "metadata": {
        "id": "AZ3NTZzHqNDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy model to SageMaker\n",
        "predictor = huggingface_model.deploy(\n",
        "\tinitial_instance_count=1, # number of instances\n",
        "\tinstance_type='ml.g5.16xlarge', #'ml.g5.4xlarge'\n",
        " \tcontainer_startup_health_check_timeout=300\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlFfGq5xqPdL",
        "outputId": "4861e36b-6d04-4df2-805b-a619d246c400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for conversation history; in a real application, this would be more dynamically managed\n",
        "conversation_history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "\n",
        "def generate_response(user_input):\n",
        "    \"\"\"\n",
        "    Generate a response from the model based on the user's input and the conversation history.\n",
        "    \"\"\"\n",
        "    global conversation_history\n",
        "    # Add the latest user input to the conversation history\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Prepare the prompt by including the conversation history\n",
        "    prompt_with_history = \"\"\n",
        "    for entry in conversation_history:\n",
        "        prompt_with_history += f\"{entry['role']}: {entry['content']}\\n\"\n",
        "    prompt_with_history += \"assistant:\"\n",
        "\n",
        "    # Hyperparameters for the LLM request, with the updated prompt\n",
        "    request = {\n",
        "        \"inputs\": prompt_with_history,\n",
        "        \"parameters\": {\n",
        "            \"do_sample\": True,\n",
        "            \"top_p\": 0.9,\n",
        "            \"temperature\": 0.7,\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"stop\": [\"\\nUser:\",\"\",\"\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Request to the endpoint\n",
        "    response = predictor.predict(request)\n",
        "\n",
        "    # Extracting model response\n",
        "    model_response = response[0][\"generated_text\"].split(\"assistant:\")[-1].strip()\n",
        "\n",
        "    # Add model response to the conversation history\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": model_response})\n",
        "\n",
        "    # Return the model's response\n",
        "    return model_response\n",
        "\n",
        "# Example usage\n",
        "user_input = \"What is the capital of Spain?\"\n",
        "assistant_response = generate_response(user_input)"
      ],
      "metadata": {
        "id": "dOkm5244qRq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(assistant_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSUiFg4-qWsE",
        "outputId": "6ed62266-5ea6-4ef7-f6aa-77a4fa5e992f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Madrid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"What is the most famous street in Madrid?\"\n",
        "assistant_response = generate_response(user_input)\n",
        "print(assistant_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z416zBLl7vG5",
        "outputId": "888f6307-606c-46a8-8729-dbebf8df4ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"What is the most famous house in Gran Street in Madrid?\"\n",
        "assistant_response = generate_response(user_input)\n",
        "print(assistant_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpLHObkA8suQ",
        "outputId": "98507986-f827-4b52-c6e1-293015e84739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Casa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"What country did I ask about above?\"\n",
        "assistant_response = generate_response(user_input)\n",
        "print(assistant_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V30bjKco89N3",
        "outputId": "e505de3c-1a3d-4f2c-8e10-697f3463455f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DELETE ENDPOINT to avoid unnecessary expenses\n",
        "predictor.delete_model()\n",
        "predictor.delete_endpoint()"
      ],
      "metadata": {
        "id": "1J3oKMNWqXZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TKbjLF_HEybC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}