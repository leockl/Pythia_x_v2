{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BV616dtWApv"
      },
      "source": [
        "### 0 - Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05hMgR3hD2NJ",
        "outputId": "29f668a8-d2b9-41a2-aad2-162d2627685f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/844.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m542.7/844.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m844.7/844.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sagemaker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for protobuf3-to-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install sagemaker python-dotenv --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-jarnrCZslB"
      },
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWyP01zgMI3r"
      },
      "source": [
        "### 1 - AWS Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG6fZtY3DtGZ",
        "outputId": "237814c7-a99d-4118-c677-542e99bcc41a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# environment variables\n",
        "# Option 1\n",
        "# os.environ[\"aws_access_key_id\"]='aws_access_key_id'\n",
        "# os.environ[\"aws_secret_access_key\"]='aws_secret_access_key'\n",
        "\n",
        "# Option 2\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHKSkn3jcxcX"
      },
      "source": [
        "  [Secret keys](https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lQjG0DGPwKf"
      },
      "outputs": [],
      "source": [
        "REGION_NAME = \"us-east-1\"\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] = REGION_NAME\n",
        "ROLE_NAME =  'Sagemaker-ExecutionRole'\n",
        "\n",
        "auth_arguments = {\n",
        "    'aws_access_key_id':os.environ[\"aws_access_key_id\"],\n",
        "    'aws_secret_access_key':os.environ[\"aws_secret_access_key\"],\n",
        "    'region_name':REGION_NAME\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDJJRkX7EMPi"
      },
      "source": [
        "[IAM rol](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geOptch-Ep-W"
      },
      "outputs": [],
      "source": [
        "iam = boto3.client('iam', **auth_arguments)\n",
        "role = iam.get_role(RoleName=ROLE_NAME)['Role']['Arn']\n",
        "\n",
        "session = sagemaker.Session(boto3.Session(**auth_arguments))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsXcwx9MMDd7"
      },
      "source": [
        "### 2 - Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVUJhmZ0EOja",
        "outputId": "5d138576-2741-4560-da36-b531215c22a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.0-tgi0.8.2-gpu-py39-cu118-ubuntu20.04\n"
          ]
        }
      ],
      "source": [
        "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
        "\n",
        "# image uri\n",
        "llm_image = get_huggingface_llm_image_uri(\"huggingface\")\n",
        "\n",
        "print(f\"image uri: {llm_image}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaWhGsUy3Cb2"
      },
      "outputs": [],
      "source": [
        "from sagemaker.huggingface import HuggingFaceModel\n",
        "\n",
        "# Falcon 7b\n",
        "hub = {'HF_MODEL_ID':'tiiuae/falcon-7b'}\n",
        "\n",
        "# Hugging Face Model Class\n",
        "huggingface_model = HuggingFaceModel(\n",
        "   env=hub,\n",
        "   role=role,  # iam role from AWS\n",
        "   image_uri=llm_image,\n",
        "   sagemaker_session=session\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEWp5OSEGuEl",
        "outputId": "7e14cc3c-ee76-4068-a16d-08c042999bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------!"
          ]
        }
      ],
      "source": [
        "# deploy model to SageMaker\n",
        "predictor = huggingface_model.deploy(\n",
        "\tinitial_instance_count=1, # number of instances\n",
        "\tinstance_type='ml.g5.2xlarge', #'ml.g5.4xlarge'\n",
        " \tcontainer_startup_health_check_timeout=300\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouZf5rK8L6Ky"
      },
      "source": [
        "### 3 - Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E8NHDnN0OI1"
      },
      "outputs": [],
      "source": [
        "# Placeholder for conversation history; in a real application, this would be more dynamically managed\n",
        "conversation_history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "\n",
        "def generate_response(user_input):\n",
        "    \"\"\"\n",
        "    Generate a response from the model based on the user's input and the conversation history.\n",
        "    \"\"\"\n",
        "    global conversation_history\n",
        "    # Add the latest user input to the conversation history\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "    \n",
        "    # Prepare the prompt by including the conversation history\n",
        "    prompt_with_history = \"\\n\".join(conversation_history) + \"\\nassistant:\"\n",
        "    \n",
        "    # Hyperparameters for the LLM request, with the updated prompt\n",
        "    request = {\n",
        "        \"inputs\": prompt_with_history,\n",
        "        \"parameters\": {\n",
        "            \"do_sample\": True,\n",
        "            \"top_p\": 0.9,\n",
        "            \"temperature\": 0.7,\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"stop\": [\"\\nUser:\",\"\",\"</s>\"]\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Request to the endpoint\n",
        "    response = predictor.predict(request)\n",
        "    \n",
        "    # Extracting model response\n",
        "    model_response = response[0][\"generated_text\"].split(\"assistant:\")[-1].strip()\n",
        "    \n",
        "    # Add model response to the conversation history\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": model_response})\n",
        "    \n",
        "    # Return the model's response\n",
        "    return model_response\n",
        "\n",
        "# Example usage\n",
        "user_input = \"What is the capital of Spain?\"\n",
        "assistant_response = generate_response(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lNOJoaf3Dtv",
        "outputId": "24e82907-0753-4ab4-e42e-e487e402d591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Madrid.\n",
            "User:\n"
          ]
        }
      ],
      "source": [
        "print(assistant_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NieX7MHHdhCQ"
      },
      "outputs": [],
      "source": [
        "# DELETE ENDPOINT to avoid unnecessary expenses\n",
        "predictor.delete_model()\n",
        "predictor.delete_endpoint()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6BV616dtWApv",
        "GWyP01zgMI3r",
        "UsXcwx9MMDd7",
        "ouZf5rK8L6Ky"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
