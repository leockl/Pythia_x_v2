{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT USING LANGCHAIN (FOR MEMORY + CONVERSATIONAL AI)\n",
    "\n",
    "#1 import the OS, Bedrock, ConversationChain, ConversationBufferMemory Langchain Modules\n",
    "import os\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "#2a Write a function for invoking model- client connection with Bedrock with profile, model_id & Inference params- model_kwargs\n",
    "# def demo_chatbot():\n",
    "def demo_chatbot(input_text):\n",
    "    demo_llm = Bedrock(\n",
    "       credentials_profile_name='default',\n",
    "       model_id='mistral.mixtral-8x7b-instruct-v0:1',\n",
    "       model_kwargs= {\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_p\": 0.5,\n",
    "        \"max_gen_len\": 512})\n",
    "    # return demo_llm\n",
    "    \n",
    "#2b Test out the LLM with Predict method\n",
    "    return demo_llm.invoke(input_text)\n",
    "# response = demo_chatbot('what is the temprature in london like ?')\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for conversation history; in a real application, this would be more dynamically managed\n",
    "conversation_history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "def generate_response(user_input):\n",
    "    \"\"\"\n",
    "    Generate a response from the model based on the user's input and the conversation history.\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "    # Add the latest user input to the conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Prepare the prompt by including the conversation history\n",
    "    prompt_with_history = \"\"\n",
    "    for entry in conversation_history:\n",
    "        prompt_with_history += f\"{entry['role']}: {entry['content']}\\n\"\n",
    "    prompt_with_history += \"assistant:\"\n",
    "\n",
    "    # Request to the endpoint\n",
    "    # response = predictor.predict(request)\n",
    "    response = demo_chatbot(prompt_with_history)\n",
    "\n",
    "    # Extracting model response\n",
    "    # model_response = response[0][\"generated_text\"].split(\"assistant:\")[-1].strip()\n",
    "    model_response = response.split(\"assistant:\")[-1].strip()\n",
    "\n",
    "    # Add model response to the conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": model_response})\n",
    "\n",
    "    # Return the model's response\n",
    "    return model_response\n",
    "\n",
    "# Example usage\n",
    "user_input = \"What is the capital of Spain?\"\n",
    "assistant_response = generate_response(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Spain is Madrid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Spain is Madrid.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(assistant_response)\n",
    "assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most famous streets in Madrid is the Gran Via. It's known for its shops, restaurants, and historic buildings. It's also a major thoroughfare in the city and a popular place for both locals and tourists.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What is the most famous street in Madrid?\"\n",
    "assistant_response = generate_response(user_input)\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There isn't a single most famous house on Gran Via Street, but one of the most notable buildings is the Metropolis Building. It's a beautiful example of Beaux-Arts architecture and is one of the most photographed buildings in Madrid. It's located at the beginning of Gran Via, near the intersection with Alcal√° Street.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What is the most famous house in Gran Via Street in Madrid?\"\n",
    "assistant_response = generate_response(user_input)\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked about Spain. All the questions you asked were related to Spain and its capital, Madrid.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What country did I ask about above?\"\n",
    "assistant_response = generate_response(user_input)\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
